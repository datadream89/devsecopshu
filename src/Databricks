import requests
import concurrent.futures

# Function to submit a job
def submit_job(job_config):
    url = "https://<YOUR-DATABRICKS-WORKSPACE-URL>/api/2.0/jobs/runs/submit"
    headers = {
        "Authorization": "Bearer <YOUR-ACCESS-TOKEN>",
        "Content-Type": "application/json"
    }
    response = requests.post(url, headers=headers, json=job_config)
    response_json = response.json()
    return response_json

# Define your job configurations
job_configs = [
    {
        "job_id": "<JOB-ID-1>",
        "notebook_task": {
            "notebook_path": "/Users/<USERNAME>/<NOTEBOOK-PATH>", # Example notebook path
            "base_parameters": {
                "param1": "value1",
                "param2": "value2"
            }
        },
        "new_cluster": {
            "spark_version": "7.3.x-scala2.12",
            "node_type_id": "i3.xlarge",
            "num_workers": 2,
            # Add more cluster configurations as needed
        }
    },
    {
        "job_id": "<JOB-ID-2>",
        "notebook_task": {
            "notebook_path": "/Users/<USERNAME>/<NOTEBOOK-PATH>", # Example notebook path
            "base_parameters": {
                "param1": "value1",
                "param2": "value2"
            }
        },
        "existing_cluster_id": "<EXISTING-CLUSTER-ID>"  # Use existing cluster
    },
    # Add more job configurations as needed
]

# Submit jobs in parallel
with concurrent.futures.ThreadPoolExecutor() as executor:
    futures = [executor.submit(submit_job, job_config) for job_config in job_configs]
    results = [future.result() for future in concurrent.futures.as_completed(futures)]

# Print results
for result in results:
    print(result)
