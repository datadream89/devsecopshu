import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.operators.s3_bucket import S3CreateBucketOperator
from airflow.providers.amazon.aws.sensors.s3_key import S3KeySensor
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator

# Define the default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 5, 2),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# Define the DAG
dag = DAG(
    dag_id='databricks_s3',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
)

# Define the S3 bucket and key
bucket_name = 'my-s3-bucket'
s3_key = 'my-s3-key'

# Create the S3 bucket if it does not exist
create_s3_bucket = S3CreateBucketOperator(
    task_id='create_s3_bucket',
    bucket_name=bucket_name,
    region_name='us-west-2',
    aws_conn_id='aws_default',
    dag=dag,
)

# Wait for the file to be uploaded to S3
s3_key_sensor = S3KeySensor(
    task_id='s3_key_sensor',
    bucket_name=bucket_name,
    bucket_key=s3_key,
    wildcard_match=False,
    aws_conn_id='aws_default',
    dag=dag,
)

# Define the Databricks cluster and job configurations
databricks_cluster_config = {
    'spark_version': '7.3.x-scala2.12',
    'node_type_id': 'i3.xlarge',
    'num_workers': 1,
    'spark_conf': {
        'spark.speculation': 'false'
    }
}
databricks_job_config = {
    'new_cluster': databricks_cluster_config,
    'notebook_task': {
        'notebook_path': '/my-notebook',
    },
    'libraries': [{
        'jar': 'dbfs:/my-libraries/my-library.jar'
    }]
}

# Submit the Databricks job
submit_databricks_job = DatabricksSubmitRunOperator(
    task_id='submit_databricks_job',
    databricks_conn_id='databricks_default',
    job_config=databricks_job_config,
    dag=dag,
)

# Set the task dependencies
create_s3_bucket >> s3_key_sensor >> submit_databricks_job
