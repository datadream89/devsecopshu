from kubernetes import client, config

def create_job(api_instance, job_name, container_name, unique_value):
    container = client.V1Container(
        name=container_name,
        image="your_container_image_here",
        env=[client.V1EnvVar(name="UNIQUE_VALUE", value=unique_value)],
    )

    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": container_name}),
        spec=client.V1PodSpec(restart_policy="Never", containers=[container]),
    )

    spec = client.V1JobSpec(template=template, backoff_limit=0)

    job = client.V1Job(
        api_version="batch/v1",
        kind="Job",
        metadata=client.V1ObjectMeta(name=job_name),
        spec=spec,
    )

    try:
        api_instance.create_namespaced_job(namespace="default", body=job)
        print(f"Job {job_name} created successfully.")
    except client.ApiException as e:
        print(f"Error creating Job: {e}")

def main():
    config.load_kube_config()

    # Replace this list with your unique values
    unique_values_list = ["value1", "value2", "value3", "value4", "value5", "value6", "value7", "value8", "value9", "value10"]

    num_jobs = 4  # Number of Jobs
    num_containers_per_job = 2  # Number of Containers per Job

    for i in range(num_jobs):
        for j in range(num_containers_per_job):
            container_name = f"container-{i}-{j}"
            unique_value = unique_values_list[i * num_containers_per_job + j]
            job_name = f"job-{i}-{j}"

            create_job(api_instance=client.BatchV1Api(), job_name=job_name, container_name=container_name, unique_value=unique_value)

if __name__ == "__main__":
    main()

# Install Scala
ENV SCALA_VERSION=2.12.9
RUN yum -y install scala-$SCALA_VERSION

# Set up Snowpark dependencies
ENV SNOWPARK_VERSION=0.11.2
RUN mkdir -p /app/libs && \
    curl -L https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/3.2.0/spark-sql_2.12-3.2.0.jar -o /app/libs/spark-sql.jar && \
    curl -L https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar -o /app/libs/spark-sql-kafka.jar && \
    curl -L https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.0/spark-avro_2.12-3.2.0.jar -o /app/libs/spark-avro.jar && \
    curl -L https://repo1.maven.org/maven2/org/scalatest/scalatest_2.12/3.2.10/scalatest_2.12-3.2.10.jar -o /app/libs/scalatest.jar

# Install additional dependencies
RUN scala -e 'libraryDependencies += "org.scalaj" %% "scalaj-http" % "2.4.2"' && \
    scala -e 'libraryDependencies += "org.json4s" %% "json4s-jackson" % "3.7.0"' && \
    scala -e 'libraryDependencies += "org.apache.spark" %% "snowpark" % "$SNOWPARK_VERSION"' && \
    scala -e 'libraryDependencies += "com.typesafe" % "config" % "1.4.1"'

WORKDIR /app

# Copy your Scala code to the container
COPY src/main/scala ./src/main/scala

# Build your Snowpark code
RUN scalac -cp "/app/libs/spark-sql.jar:/app/libs/spark-sql-kafka.jar:/app/libs/spark-avro.jar:/app/libs/scalatest.jar" src/main/scala/*.scala

CMD ["scala", "-cp", "/app/libs/spark-sql.jar:/app/libs/spark-sql-kafka.jar:/app/libs/spark-avro.jar:/app/libs/scalatest.jar", "YourSnowparkCode"]
