FROM registry.access.redhat.com/ubi8/ubi:latest AS build
from kubernetes import client, config

# Assuming you have a list of values
my_list = ['value1', 'value2', 'value3', 'value4']

# Load the Kubernetes configuration
config.load_kube_config()

# Create the Kubernetes API client
api_client = client.ApiClient()

# Create a list to store the Job objects
jobs = []

# Create jobs for every three values in the list
for i in range(0, len(my_list), 3):
    # Get three values for the current job
    values = my_list[i:i+3]

    # Create the Job specification
    job_spec = {
        "apiVersion": "batch/v1",
        "kind": "Job",
        "metadata": {
            "name": f"my-job-{i}"
        },
        "spec": {
            "template": {
                "spec": {
                    "restartPolicy": "Never",
                    "containers": []
                }
            }
        }
    }

    # Create a container for each value in the current job
    for j, value in enumerate(values):
        container = {
            "name": f"container-{j}",
            "image": "your-container-image",
            "env": [
                {
                    "name": "MY_ENV_VARIABLE",
                    "value": value
                }
            ]
        }
        job_spec["spec"]["template"]["spec"]["containers"].append(container)

    # Add the Job to the list
    jobs.append(job_spec)

# Create the Job objects
api_instance = client.BatchV1Api(api_client)
for job_spec in jobs:
    api_instance.create_namespaced_job(body=job_spec, namespace="default")

# Install Java
RUN yum -y install java-11-openjdk-devel

# Install Scala
ENV SCALA_VERSION=2.12.9
RUN yum -y install scala-$SCALA_VERSION

# Set up Snowpark dependencies
ENV SNOWPARK_VERSION=0.11.2
RUN mkdir -p /app/libs && \
    curl -L https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/3.2.0/spark-sql_2.12-3.2.0.jar -o /app/libs/spark-sql.jar && \
    curl -L https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar -o /app/libs/spark-sql-kafka.jar && \
    curl -L https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.0/spark-avro_2.12-3.2.0.jar -o /app/libs/spark-avro.jar && \
    curl -L https://repo1.maven.org/maven2/org/scalatest/scalatest_2.12/3.2.10/scalatest_2.12-3.2.10.jar -o /app/libs/scalatest.jar

# Install additional dependencies
RUN scala -e 'libraryDependencies += "org.scalaj" %% "scalaj-http" % "2.4.2"' && \
    scala -e 'libraryDependencies += "org.json4s" %% "json4s-jackson" % "3.7.0"' && \
    scala -e 'libraryDependencies += "org.apache.spark" %% "snowpark" % "$SNOWPARK_VERSION"' && \
    scala -e 'libraryDependencies += "com.typesafe" % "config" % "1.4.1"'

WORKDIR /app

# Copy your Scala code to the container
COPY src/main/scala ./src/main/scala

# Build your Snowpark code
RUN scalac -cp "/app/libs/spark-sql.jar:/app/libs/spark-sql-kafka.jar:/app/libs/spark-avro.jar:/app/libs/scalatest.jar" src/main/scala/*.scala

CMD ["scala", "-cp", "/app/libs/spark-sql.jar:/app/libs/spark-sql-kafka.jar:/app/libs/spark-avro.jar:/app/libs/scalatest.jar", "YourSnowparkCode"]
