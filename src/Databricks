from pyspark.sql import SparkSession
from pyspark.sql.functions import rand

# Create a SparkSession
spark = SparkSession.builder \
    .appName("SyntheticDataGeneration") \
    .getOrCreate()

# Load your original dataset
original_data = spark.read.csv("original_data.csv", header=True, inferSchema=True)

# Calculate the counts for each distinct value in each column
value_counts = {}
for column in original_data.columns:
    value_counts[column] = original_data.groupBy(column).count()

# Define the number of records in the synthetic dataset
desired_record_count = 1000000  # Adjust this number based on your requirements

# Generate synthetic data
synthetic_data = None
for column in original_data.columns:
    # Select values with replacement based on the frequency distribution
    sampled_values = value_counts[column].sample(withReplacement=True, fraction=desired_record_count/original_data.count(), seed=42)
    
    # Select random values from the sampled values for the column
    synthetic_column = sampled_values.orderBy(rand()).limit(desired_record_count)
    
    # Combine the synthetic column with existing synthetic data
    if synthetic_data is None:
        synthetic_data = synthetic_column.withColumnRenamed("count", column)
    else:
        synthetic_data = synthetic_data.crossJoin(synthetic_column.withColumnRenamed("count", column))

# Shuffle the DataFrame to randomize the order of rows
synthetic_data = synthetic_data.orderBy(rand())

# Save the synthetic data
synthetic_data.write.csv("synthetic_data.csv", header=True, mode="overwrite")

# Stop the SparkSession
spark.stop()
